NN.add_layer(1,1,identity) -> NN.add_layer(1,10,sigmoid)

H = -2 * (training_labels[j,:] - output)
->
H = output - training_labels[j,:]

yop guys j'ai fix cette erreur en éditant : 
layer.gradient_weights += H * temp @ layer.entry
-> layer.gradient_weights += np.outer(H * temp, layer.entry)


questions : 
Si la perte fait 
4664.421111951798
63.2247659823789
10.366655346406102
8.738356722185213
8.687675419949016
8.686096778358948
8.686047600897757
8.686046068905304
8.686046021180024
8.686046019693261

ou cela avec d'autre paramètres 
2059.9954381836033
983.0934748609003
523.5192901647328
310.34667924399974
202.687429275017
142.97577269971097
106.55503297464661
82.400865426283
65.32432760918401
52.71421731801441
43.14348173164526
35.75890263773439
30.005741958647196
25.4982114150112
21.95488603001631
19.16398930185574
16.963071828181377
15.226080332040015
13.854531024020643
12.771163233648892
11.915213771703467
11.238819045864384
10.704238083314923
10.281691222789622
9.947669356003965
...
8.686046077743423
8.686046065594104
8.68604605598541
8.686046048386075

cela veut dire que l'on atteint la fonction avec la perte minimale pour le degré du polynome donné ?