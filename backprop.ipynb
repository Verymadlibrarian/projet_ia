import numpy as np
import matplotlib.pyplot as plt

# Définition d'une classe activation simplifiée
class activation:
    def __init__(self, function, derivative):
        self.function = function
        self.derivative = derivative

# Fonctions d'activation
sigmoid = activation(lambda x: 1/(1+np.exp(-x)),
                     lambda x: np.exp(-x)/((1+np.exp(-x))**2))

def softmax(x):
    ex = np.exp(x - np.max(x))
    return ex / np.sum(ex)

# On n'utilisera pas explicitement la dérivée de softmax car la combinaison softmax/cross-entropy est simple.
softmax_act = activation(softmax, None)

identity = activation(lambda x: x, lambda x: 1)

# Définition d'une couche du réseau
class layer:
    def __init__(self, size_in, size_out, activation_function):
        self.entry = np.zeros([size_in, 1])  # vecteur colonne
        self.weights = np.random.normal(size=(size_out, size_in))
        self.gradient_weights = np.zeros((size_out, size_in))
        self.bias = np.random.normal(size=(size_out, 1))
        self.gradient_bias = np.zeros((size_out, 1))
        self.activation = activation_function
    
    def output(self):
        # Calcul linéaire suivi de l'activation
        z = self.weights @ self.entry + self.bias
        return self.activation.function(z)
    
    def get_z(self):
        # Retourne le résultat linéaire non activé
        return self.weights @ self.entry + self.bias
    
    def derivate(self):
        # Calcule la dérivée de l'activation (si besoin)
        z = self.get_z()
        return self.activation.derivative(z)
    
    def descend(self, step_length):
        self.weights -= step_length * self.gradient_weights
        self.bias -= step_length * self.gradient_bias
        # Réinitialiser les gradients après la mise à jour
        self.gradient_weights = np.zeros(self.weights.shape)
        self.gradient_bias = np.zeros(self.bias.shape)

# Définition du réseau de neurones
class network:
    def __init__(self):
        self.layers = []
        
    def add_layer(self, size_in, size_out, activation_function):
        new_layer = layer(size_in, size_out, activation_function)
        self.layers.append(new_layer)
        
    def run(self, x):
        # On suppose que x est un vecteur colonne
        output = np.copy(x)
        for layer in self.layers:
            layer.entry = output.copy()
            output = layer.activation.function(layer.weights @ layer.entry + layer.bias)
        return output
    
    def fit(self, training_data, training_labels, nb_steps, step_length):
        n = training_data.shape[0]
        for i in range(nb_steps):
            loss = 0
            for j in range(n):
                # Pour chaque exemple, transformer l'entrée en vecteur colonne
                x = training_data[j, :].reshape(-1, 1)
                output = self.run(x)
                # Pour la classification softmax + cross-entropy
                # La dérivée simplifiée est (output - label)
                target = training_labels[j, :].reshape(-1, 1)
                H = output - target
                loss += -np.sum(target * np.log(output + 1e-8))  # cross-entropy

                # Rétropropagation
                for layer in reversed(self.layers):
                    # Si la couche utilise softmax en sortie et qu’on combine avec cross-entropy,
                    # le gradient a déjà été calculé comme H = output - target.
                    # Pour les couches cachées, on utilise la dérivée de l'activation.
                    if layer.activation.derivative is not None:
                        temp = layer.derivate()
                        layer.gradient_weights += (H * temp) @ layer.entry.transpose()
                        layer.gradient_bias += H * temp
                        H = layer.weights.transpose() @ (H * temp)
                    else:
                        # Par exemple, pour la couche de sortie avec softmax,
                        # on n'a pas besoin d'appliquer la dérivée.
                        layer.gradient_weights += H @ layer.entry.transpose()
                        layer.gradient_bias += H
                        H = layer.weights.transpose() @ H
                    
            # Mise à jour des paramètres pour toutes les couches
            for layer in self.layers:
                layer.descend(step_length)
            
            if i % 100 == 0:
                print("Loss à l'étape", i, ":", loss)

# ===========================================================================
# Exemple d'utilisation pour la classification (chiffres MNIST par exemple)
# ===========================================================================
# Ici, on suppose que training_images est un tableau de forme (n_ex, 784)
# et training_labels est de forme (n_ex, 10) avec un encodage one-hot.
# Pour cet exemple, nous utilisons des données fictives.

n_ex = 100  # nombre d'exemples d'entraînement fictifs
input_size = 784  # 28x28 pixels
num_classes = 10

# Création de données fictives
training_images = np.random.rand(n_ex, input_size)
# Encodage one-hot aléatoire
training_labels = np.zeros((n_ex, num_classes))
for i in range(n_ex):
    random_class = np.random.randint(0, num_classes)
    training_labels[i, random_class] = 1

# Création du réseau
NN = network()
# Première couche (entrée) : de 784 vers 128 neurones avec activation sigmoid (par exemple)
NN.add_layer(input_size, 128, sigmoid)
# Couche intermédiaire cachée (facultative)
NN.add_layer(128, 64, sigmoid)
# Couche de sortie : 64 vers 10 neurones avec activation softmax
NN.add_layer(64, num_classes, softmax_act)

# Entraînement du réseau
NN.fit(training_images, training_labels, nb_steps=1000, step_length=1e-4)

# Prédictions sur des exemples d'entraînement
hatY = np.zeros((n_ex, num_classes))
for j in range(n_ex):
    x = training_images[j, :].reshape(-1, 1)
    hatY[j, :] = NN.run(x).flatten()

# Pour visualiser la performance, on peut comparer la classe prédite à la classe cible
predicted_labels = np.argmax(hatY, axis=1)
true_labels = np.argmax(training_labels, axis=1)

print("Précision sur l'échantillon d'entraînement :", np.mean(predicted_labels == true_labels))
